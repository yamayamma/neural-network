{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "K=256, CelebAデータセットtraining train size112*80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R-8-owwX5D8G"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "VQ-VAE\n",
            "epoch: 1/20 recon_error: 0.012762678121986587\n",
            "epoch: 2/20 recon_error: 0.0031913566208759867\n",
            "epoch: 3/20 recon_error: 0.0025013645608564766\n",
            "epoch: 4/20 recon_error: 0.002197477231022176\n",
            "epoch: 5/20 recon_error: 0.0020023070448114883\n",
            "epoch: 6/20 recon_error: 0.001787105417355719\n",
            "epoch: 7/20 recon_error: 0.0015869953156280236\n",
            "epoch: 8/20 recon_error: 0.001469552579269869\n",
            "epoch: 9/20 recon_error: 0.0014103983780075833\n",
            "Model saved.\n",
            "epoch: 10/20 recon_error: 0.0013526682243279115\n",
            "epoch: 11/20 recon_error: 0.0013117835211648246\n",
            "epoch: 12/20 recon_error: 0.0012764840454640468\n",
            "epoch: 13/20 recon_error: 0.0012398473441776797\n",
            "epoch: 14/20 recon_error: 0.0011945654979107652\n",
            "epoch: 15/20 recon_error: 0.0011428679471586164\n",
            "epoch: 16/20 recon_error: 0.0011186836441377487\n",
            "epoch: 17/20 recon_error: 0.001097555080425053\n",
            "epoch: 18/20 recon_error: 0.0010837421436266108\n",
            "epoch: 19/20 recon_error: 0.0010726617455783145\n",
            "Model saved.\n",
            "epoch: 20/20 recon_error: 0.001064575422008881\n",
            "test\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "batch_size = 256\n",
        "# VQ-VAE\n",
        "VQ_epoch = 20 # epoch\n",
        "# default\n",
        "embedding_dim = 64 # D\n",
        "num_embeddings = 256 # K\n",
        "num_hiddens = 128 # out_channels\n",
        "num_residual_hiddens = 32 # residualのout_channels\n",
        "num_residual_layers = 2 # residualの繰り返し数\n",
        "commitment_cost = 0.25\n",
        "decay = 0 # EMA使うか\n",
        "learning_rate = 1e-3\n",
        "\n",
        "img_size = (56*2, 40*2) # 中心からimg_size*img_size\n",
        "in_channels=color=3 # カラー画像\n",
        "img_num = 25 # 画像の枚数\n",
        "folder = './VQ_VAE' # 保存場所\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "# dataset\n",
        "# train 162770 test 19962 valid 19867 all 202599 , size 3, 218, 178\n",
        "training_data = datasets.CelebA(root=\"~/b2022_kodai/data\", download=True, split= 'train', \n",
        "                                  transform=transforms.Compose([\n",
        "                                    transforms.CenterCrop((img_size[0], img_size[1])),\n",
        "                                    # transforms.Resize(img_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0)) # torchvision.transforms.Normalize(mean, std, inplace=False)\n",
        "                                  ])) # テンソル　正規化 \n",
        "\n",
        "validation_data = datasets.CelebA(root=\"~/b2022_kodai/data\",  download=True, split= 'valid',# target_type='identity',\n",
        "                                  transform=transforms.Compose([\n",
        "                                    transforms.CenterCrop((img_size[0], img_size[1])),\n",
        "                                    # transforms.Resize(img_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
        "                                  ]))\n",
        "\n",
        "training_loader = DataLoader(training_data, batch_size=batch_size, drop_last=True, shuffle=True, pin_memory=True)\n",
        "validation_loader = DataLoader(validation_data, batch_size=batch_size, drop_last=True, shuffle=True, pin_memory=True)\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost): # K, D, beta\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "        \n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._num_embeddings = num_embeddings\n",
        "        \n",
        "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim) # torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, device=None, dtype=None)\n",
        "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings) # 一様分布\n",
        "        self._commitment_cost = commitment_cost # 0.25 ?\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # convert inputs from BCHW -> BHWC\n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous() # 軸の入れ替え　参照ではない\n",
        "        input_shape = inputs.shape\n",
        "        \n",
        "        # Flatten input (16384, 64)\n",
        "        flat_input = inputs.view(-1, self._embedding_dim)\n",
        "        \n",
        "        # Calculate distances 2乗距離 (16384, 512)\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) + torch.sum(self._embedding.weight**2, dim=1) - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
        "            \n",
        "        # Encoding　One-hot表現\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1) # 最短距離のindex\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1) # Tensor.scatter_(dim, index, src, reduce=None)\n",
        "        \n",
        "        # Quantize and unflatten 埋め込みベクトルに置き換え\n",
        "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
        "        quantized2 = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
        "        \n",
        "        # Loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs) # 第2項\n",
        "        q_latent_loss = F.mse_loss(quantized, inputs.detach()) # 第3項\n",
        "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
        "        \n",
        "        quantized = inputs + (quantized - inputs).detach() # 勾配の切り離し\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "        \n",
        "        # convert quantized from BHWC -> BCHW\n",
        "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, distances ,encoding_indices, encodings\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens): # 128, 128, 32\n",
        "        super(Residual, self).__init__()\n",
        "        self._block = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=num_residual_hiddens, kernel_size=3, stride=1, padding=1, bias=False), # チャンネル数32に減らす\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=num_residual_hiddens, out_channels=num_hiddens, kernel_size=1, stride=1, bias=False) # チャンネル数128に戻す\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x + self._block(x)\n",
        "\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):# 128, 128, 2, 32\n",
        "        super(ResidualStack, self).__init__()\n",
        "        self._num_residual_layers = num_residual_layers\n",
        "        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens) for _ in range(self._num_residual_layers)]) # residual 2回\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self._num_residual_layers):\n",
        "            x = self._layers[i](x)\n",
        "        return F.relu(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens): # 3, 128, 2, 32\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self._conv_1 = nn.Conv2d(in_channels=in_channels, out_channels=num_hiddens//2, kernel_size=4, stride=2, padding=1) # チャンネル数64に増やす サイズ1/2\n",
        "        self._conv_2 = nn.Conv2d(in_channels=num_hiddens//2, out_channels=num_hiddens, kernel_size=4, stride=2, padding=1) # チャンネル数128に増やす サイズ1/2\n",
        "        self._conv_3 = nn.Conv2d(in_channels=num_hiddens, out_channels=num_hiddens, kernel_size=3, stride=1, padding=1) # 維持\n",
        "        self._residual_stack = ResidualStack(in_channels=num_hiddens, num_hiddens=num_hiddens, num_residual_layers=num_residual_layers, num_residual_hiddens=num_residual_hiddens)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self._conv_1(inputs)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self._conv_2(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self._conv_3(x)\n",
        "        return self._residual_stack(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens): # D, 128, 2, 32\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self._conv_1 = nn.Conv2d(in_channels=in_channels, out_channels=num_hiddens, kernel_size=3, stride=1, padding=1) # D, 128\n",
        "        \n",
        "        self._residual_stack = ResidualStack(in_channels=num_hiddens, num_hiddens=num_hiddens, num_residual_layers=num_residual_layers, num_residual_hiddens=num_residual_hiddens) # 128, 128, 2, 32\n",
        "        \n",
        "        self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens, out_channels=num_hiddens//2, kernel_size=4, stride=2, padding=1) # 128, 64 サイズ2倍\n",
        "        # ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)\n",
        "        self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens//2, out_channels=color, kernel_size=4, stride=2, padding=1) # 64, channel数 サイズ2倍(元のサイズ)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self._conv_1(inputs)\n",
        "        \n",
        "        x = self._residual_stack(x)\n",
        "        \n",
        "        x = self._conv_trans_1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        return self._conv_trans_2(x)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens, \n",
        "                 num_embeddings, embedding_dim, commitment_cost, decay=0):\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "        self._encoder = Encoder(in_channels, num_hiddens, num_residual_layers, num_residual_hiddens) # 3, 128, 2, 32\n",
        "        self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens, out_channels=embedding_dim, kernel_size=1, stride=1) # 128, 64\n",
        "        # if decay > 0.0:\n",
        "        #     self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim, \n",
        "        #                                       commitment_cost, decay)\n",
        "        # else:\n",
        "        self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost) # K, D, beta\n",
        "        self._decoder = Decoder(embedding_dim, num_hiddens, num_residual_layers, num_residual_hiddens) # D, 128, 2, 32\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self._encoder(x) # channel=128, size=1/4\n",
        "        z = self._pre_vq_conv(z) # channel=64 埋め込みベクトルの次元にする\n",
        "        loss, quantized, perplexity, distances, encoding_indices, encodings = self._vq_vae(z) # 埋め込みベクトルに置き換え\n",
        "        x_recon = self._decoder(quantized)\n",
        "\n",
        "        return loss, x_recon, perplexity, distances, encoding_indices, encodings\n",
        "\n",
        "model = Model(in_channels, num_hiddens, num_residual_layers, num_residual_hiddens, num_embeddings, embedding_dim, commitment_cost, decay).to(device) # モデル作成\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)\n",
        "model.train()\n",
        "train_recon_error = [] # エポック毎にMSEを記録\n",
        "\n",
        "print('VQ-VAE')\n",
        "# train\n",
        "for i in range(VQ_epoch):\n",
        "    # print(i)\n",
        "    train_recon = []\n",
        "    for data, _ in training_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        vq_loss, data_recon, perplexity, distances, encoding_indices, encodings = model(data)\n",
        "        recon_error = F.mse_loss(data_recon, data) # データ毎の損失\n",
        "        loss = recon_error + vq_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_recon.append(recon_error.item())  \n",
        "    if (i+1) % 10 == 0:\n",
        "        torch.save(model.to('cpu').state_dict() , '{}/VQ-VAE_epoch_{}.pth'.format(folder, i+1))\n",
        "        print('Model saved.')\n",
        "        \n",
        "    train_recon_error.append(np.mean(train_recon)) # 1エポックの平均\n",
        "    print(f'epoch: {i+1}/{VQ_epoch} recon_error: {train_recon_error[-1]}')\n",
        "\n",
        "# test\n",
        "model.eval()\n",
        "valid_error = []\n",
        "print('test')\n",
        "with torch.no_grad():\n",
        "    for valid_originals, _ in validation_loader:\n",
        "        valid_originals = valid_originals.to(device)\n",
        "        vq_output_eval = model._pre_vq_conv(model._encoder(valid_originals)) # エンコード\n",
        "        _, valid_quantize, _, _, encoding_indices, _, = model._vq_vae(vq_output_eval) # VQベクトル\n",
        "        valid_reconstructions = model._decoder(valid_quantize) # 再構成\n",
        "        valid_loss = F.mse_loss(valid_reconstructions, valid_originals)\n",
        "        valid_error.append(valid_loss.item())\n",
        "valid_error = np.mean(valid_error)\n",
        "\n",
        "save_image(valid_originals[:img_num], \"{}/k_{}original.png\".format(folder, num_embeddings), nrow=5, padding=0,  normalize=True)\n",
        "save_image(valid_reconstructions[:img_num], \"{}/k_{}recon.png\".format(folder, num_embeddings), nrow=5, padding=0,  normalize=True)\n",
        "\n",
        "df = pd.DataFrame(train_recon_error, columns=['recon'])\n",
        "df.to_csv('{}/VQ-VAE_recon.csv'.format(folder))\n",
        "df = pd.DataFrame([valid_error], columns=[\"VQ-VAE_recon_error\"])\n",
        "df.to_csv('{}/valid.csv'.format(folder))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f = plt.figure() #  (default: [6.4, 4.8]) Width, height in inches.\n",
        "ax = f.add_subplot(1,1,1) # 1行2列の1つめ\n",
        "ax.plot(train_recon_error, marker=\"o\")\n",
        "# ax.set_yscale('')\n",
        "ax.set_title('VQ-VAE MSE')\n",
        "ax.set_xlabel('epoch')\n",
        "plt.savefig('mse.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "vq-vae.ipynb のコピー",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
