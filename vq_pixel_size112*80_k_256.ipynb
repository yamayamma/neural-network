{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "11/30 VQ-VAE+gatedPixelCNN　K=64, training train, epoch50 size(112,80) kernel_a=15, kernel_b=15 VQ-VAE保存パラメータ読み込み test shuffle=False  \n",
        "予測の正しさを確認 テストデータで確認する インデックスの場所毎の一致度確認 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "record = 'model_naive_k_256' # 何についてデータ取るのか"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R-8-owwX5D8G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os \n",
        "import seaborn as sns\n",
        "import datetime\n",
        "plt.rcParams['savefig.facecolor']='white'\n",
        "plt.rcParams['savefig.edgecolor']='azure'\n",
        "plt.rcParams['axes.facecolor']='white'\n",
        "plt.rcParams['axes.edgecolor']='azure'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "from vq_vae_model import *\n",
        "from naive_pixelCNN import *\n",
        "# from GatedPixelCNN import *\n",
        "# from spot_pixelCNN import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Pn_SSZyp5D8H"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xJiuFtKa5D8N"
      },
      "outputs": [],
      "source": [
        "batch_size = 512\n",
        "# VQ-VAE\n",
        "embedding_dim = 64 # D\n",
        "num_embeddings = 256 # K\n",
        "num_hiddens = 128 # encoderのout_channels\n",
        "num_residual_hiddens = 32 # residualのout_channels\n",
        "num_residual_layers = 2 # residualの繰り返し数\n",
        "commitment_cost = 0.25\n",
        "decay = 0 # EMA使うか\n",
        "# learning_rate = 1e-3\n",
        "in_channels=color=3\n",
        "\n",
        "# Pixel CNN\n",
        "num_of_layers = 7 # マスクBの畳み込み層の数\n",
        "num_of_channels = 32 # チャネル数\n",
        "kernel_a = 7\n",
        "kernel_b = 7\n",
        "\n",
        "# Gated Pixel CNN\n",
        "nlayers=12 # number of layers for pixelcnn\n",
        "inchans=1 #number of input channels (currently only one is supported)\n",
        "nfeats=16 #number of feature maps across the network\n",
        "Klevels=num_embeddings\n",
        "\n",
        "num_of_epochs = 50 # epoch\n",
        "img_size = (112, 80) # H*W\n",
        "img_num = 25 # 画像の枚数\n",
        "d_today = datetime.date.today()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa4YXzqw5D8H"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lMckwhhx5D8I",
        "outputId": "15d9859c-9441-4cad-e2af-663888fc6c97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# train 162770 test 19962 valid 19867 all 202599 , size 3, 218, 178\n",
        "training_data = datasets.CelebA(root=\"~/b2022_kodai/data\", download=True, split= 'train', \n",
        "                                  transform=transforms.Compose([\n",
        "                                    transforms.CenterCrop((img_size[0], img_size[1])) \n",
        "                                    if img_size[0] >= 109 or img_size[1] >= 89\n",
        "                                    else transforms.CenterCrop((img_size[0]*2, img_size[1]*2)),\n",
        "                                    transforms.Resize(img_size),\n",
        "                                    transforms.ToTensor(),\n",
        "                                  ]))\n",
        "\n",
        "validation_data = datasets.CelebA(root=\"~/b2022_kodai/data\",  download=True, split= 'valid',# target_type='identity',\n",
        "                                  transform=transforms.Compose([\n",
        "                                    transforms.CenterCrop((img_size[0], img_size[1])) \n",
        "                                    if img_size[0] >= 109 or img_size[1] >= 89\n",
        "                                    else transforms.CenterCrop((img_size[0]*2, img_size[1]*2)),\n",
        "                                    transforms.Resize(img_size),\n",
        "                                    transforms.ToTensor(),\n",
        "                                  ]))\n",
        "\n",
        "# generation_data = datasets.CelebA(root=\"~/b2022_kodai/data\",  download=True, split= 'test',# target_type='identity',\n",
        "#                                   transform=transforms.Compose([\n",
        "#                                     transforms.CenterCrop((img_size[0], img_size[1])) \n",
        "#                                     if img_size[0] >= 109 or img_size[1] >= 89\n",
        "#                                     else transforms.CenterCrop((img_size[0]*2, img_size[1]*2)),\n",
        "#                                     transforms.Resize(img_size),\n",
        "#                                     transforms.ToTensor(),\n",
        "#                                   ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_loader = DataLoader(training_data, batch_size=batch_size, drop_last=True, shuffle=True, pin_memory=True)\n",
        "training_eval_loader = DataLoader(training_data, batch_size=batch_size, drop_last=True, shuffle=False, pin_memory=True)\n",
        "validation_loader = DataLoader(validation_data, batch_size=batch_size, drop_last=True, shuffle=False, pin_memory=True)\n",
        "# generation_loader = DataLoader(generation_data, batch_size=batch_size, drop_last=True, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Model(\n",
              "  (_encoder): Encoder(\n",
              "    (_conv_1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (_conv_2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (_conv_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (_residual_stack): ResidualStack(\n",
              "      (_layers): ModuleList(\n",
              "        (0): Residual(\n",
              "          (_block): Sequential(\n",
              "            (0): ReLU(inplace=True)\n",
              "            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (2): ReLU(inplace=True)\n",
              "            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          )\n",
              "        )\n",
              "        (1): Residual(\n",
              "          (_block): Sequential(\n",
              "            (0): ReLU(inplace=True)\n",
              "            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (2): ReLU(inplace=True)\n",
              "            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (_pre_vq_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (_vq_vae): VectorQuantizer(\n",
              "    (_embedding): Embedding(256, 64)\n",
              "  )\n",
              "  (_decoder): Decoder(\n",
              "    (_conv_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (_residual_stack): ResidualStack(\n",
              "      (_layers): ModuleList(\n",
              "        (0): Residual(\n",
              "          (_block): Sequential(\n",
              "            (0): ReLU(inplace=True)\n",
              "            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (2): ReLU(inplace=True)\n",
              "            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          )\n",
              "        )\n",
              "        (1): Residual(\n",
              "          (_block): Sequential(\n",
              "            (0): ReLU(inplace=True)\n",
              "            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (2): ReLU(inplace=True)\n",
              "            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (_conv_trans_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (_conv_trans_2): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Model(in_channels, num_hiddens, num_residual_layers, num_residual_hiddens, num_embeddings, embedding_dim, commitment_cost, decay).to(device)\n",
        "model_path = f'1.experiment/VQ-VAE_CelebA_train/VQ_VAE_size112*80/K_change_new/K_256/VQ-VAE_epoch_20.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PixelCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PixelCNN\n",
            "epoch: 0/50 train error: 4.643\n",
            "match:4.782549674181562%\n",
            "mse_original:0.058708172879720986\n",
            "mse_reconstructions:0.057348830135245076\n",
            "epoch: 1/50 train error: 4.210\n",
            "epoch: 2/50 train error: 3.985\n",
            "epoch: 3/50 train error: 3.814\n",
            "epoch: 4/50 train error: 3.705\n",
            "epoch: 5/50 train error: 3.637\n",
            "epoch: 6/50 train error: 3.584\n",
            "epoch: 7/50 train error: 3.539\n",
            "epoch: 8/50 train error: 3.506\n",
            "epoch: 9/50 train error: 3.481\n",
            "epoch: 10/50 train error: 3.463\n",
            "Model saved.\n",
            "match:14.672411115545975%\n",
            "mse_original:0.018898351804206247\n",
            "mse_reconstructions:0.01770959564141537\n",
            "epoch: 11/50 train error: 3.448\n",
            "epoch: 12/50 train error: 3.428\n",
            "epoch: 13/50 train error: 3.425\n",
            "epoch: 14/50 train error: 3.412\n",
            "epoch: 15/50 train error: 3.403\n",
            "epoch: 16/50 train error: 3.393\n",
            "epoch: 17/50 train error: 3.388\n",
            "epoch: 18/50 train error: 3.376\n",
            "epoch: 19/50 train error: 3.376\n",
            "epoch: 20/50 train error: 3.369\n",
            "Model saved.\n",
            "match:16.08688474485749%\n",
            "mse_original:0.01415944192558527\n",
            "mse_reconstructions:0.01297861443048245\n",
            "epoch: 21/50 train error: 3.372\n",
            "epoch: 22/50 train error: 3.363\n",
            "epoch: 23/50 train error: 3.359\n",
            "epoch: 24/50 train error: 3.354\n",
            "epoch: 25/50 train error: 3.350\n",
            "epoch: 26/50 train error: 3.344\n",
            "epoch: 27/50 train error: 3.345\n",
            "epoch: 28/50 train error: 3.338\n",
            "epoch: 29/50 train error: 3.338\n",
            "epoch: 30/50 train error: 3.337\n",
            "Model saved.\n",
            "match:16.49790386620321%\n",
            "mse_original:0.013619420510765753\n",
            "mse_reconstructions:0.012410112043940708\n",
            "epoch: 31/50 train error: 3.333\n",
            "epoch: 32/50 train error: 3.335\n",
            "epoch: 33/50 train error: 3.329\n",
            "epoch: 34/50 train error: 3.333\n",
            "epoch: 35/50 train error: 3.328\n",
            "epoch: 36/50 train error: 3.324\n",
            "epoch: 37/50 train error: 3.321\n",
            "epoch: 38/50 train error: 3.326\n",
            "epoch: 39/50 train error: 3.323\n",
            "epoch: 40/50 train error: 3.317\n",
            "Model saved.\n",
            "match:15.711192747479991%\n",
            "mse_original:0.013235794257764754\n",
            "mse_reconstructions:0.012056998810485789\n",
            "epoch: 41/50 train error: 3.317\n",
            "epoch: 42/50 train error: 3.316\n",
            "epoch: 43/50 train error: 3.311\n",
            "epoch: 44/50 train error: 3.318\n",
            "epoch: 45/50 train error: 3.305\n",
            "epoch: 46/50 train error: 3.309\n",
            "epoch: 47/50 train error: 3.310\n",
            "epoch: 48/50 train error: 3.301\n",
            "epoch: 49/50 train error: 3.302\n",
            "epoch: 50/50 train error: 3.305\n",
            "Model saved.\n",
            "match:16.33089690616256%\n",
            "mse_original:0.014228238037934429\n",
            "mse_reconstructions:0.012983671928706923\n",
            "CPU times: user 1h 30min 39s, sys: 5min 30s, total: 1h 36min 9s\n",
            "Wall time: 1h 2min 8s\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%time\n",
        "folder = f'./{d_today}/{record}/K={num_embeddings}_size{img_size}' # 保存場所\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "pixel_cnn = PixelCNN(num_of_channels, num_of_layers, num_embeddings, kernel_a, kernel_b).to(device)\n",
        "# pixel_cnn = PixelCNN(nlayers=nlayers, in_channels=inchans, nfeats=nfeats, Klevels=Klevels).to(device)\n",
        "optimizer = optim.Adam(list(pixel_cnn.parameters()))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_losses = []\n",
        "# test_losses = []\n",
        "print('PixelCNN')\n",
        "\n",
        "for epoch in range(num_of_epochs+1):\n",
        "    # 学習\n",
        "    train_errors = []\n",
        "    pixel_cnn.train()\n",
        "    for x, _ in training_loader:\n",
        "        x = x.to(device)\n",
        "        vq_output_eval = model._pre_vq_conv(model._encoder(x))\n",
        "        _, data_recon, _, _, encoding_indices, encodings = model._vq_vae(vq_output_eval)\n",
        "        target = encoding_indices.view(batch_size, data_recon.shape[2], data_recon.shape[3])\n",
        "        x = target.unsqueeze_(1).to(device)\n",
        "        loss = criterion(pixel_cnn(x.float()), target.squeeze())\n",
        "        train_errors.append(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print(f'epoch: {epoch}/{num_of_epochs} train error: {np.mean(train_errors):0.3f}')\n",
        "    train_losses.append(np.mean(train_errors))\n",
        "    if epoch % 10 == 0:\n",
        "        if epoch != 0:\n",
        "            torch.save(pixel_cnn.to('cpu').state_dict() , '{}/K={}_VQ-pixelCNN_epoch_{}.pth'.format(folder, num_embeddings ,epoch))\n",
        "            print('Model saved.')\n",
        "            pixel_cnn.to(device)\n",
        "\n",
        "        # 再構成\n",
        "        pixel_cnn.eval()\n",
        "        test_error = []\n",
        "        eval_img_max_loss_recon = [] # 再構成との誤差\n",
        "        # validation\n",
        "        eval_img_max_loss = [] # 入力画像との誤差\n",
        "        index_match_degree = [] # indexの一致度\n",
        "        match_one_hot = 0 # 一致したインデックスの内容\n",
        "        with torch.no_grad():\n",
        "            for x, _ in validation_loader:\n",
        "                eval_originals = x.to(device)\n",
        "                vq_output_eval = model._pre_vq_conv(model._encoder(eval_originals))\n",
        "                _, eval_quantize, _, _, encoding_indices, encodings = model._vq_vae(vq_output_eval)\n",
        "                eval_target = encoding_indices.view(batch_size, eval_quantize.shape[2], eval_quantize.shape[3])\n",
        "                eval_target.unsqueeze_(1).to(device)\n",
        "                \n",
        "                sample = eval_target.float()\n",
        "                sample_max = eval_target.float()\n",
        "\n",
        "                out = pixel_cnn(sample).to(device)\n",
        "                probs = F.softmax(out, dim=1)\n",
        "                # max\n",
        "                sample_max = torch.argmax(probs, 1).float().unsqueeze(1)\n",
        "                sample_max = sample_max.long()\n",
        "\n",
        "                # generate_max = decode_index(sample_max)\n",
        "                \n",
        "                b, c, h, w = sample.shape\n",
        "                sample_max_onehot = sample_max.view(-1).unsqueeze(1)\n",
        "                encodings = torch.zeros(sample_max_onehot.shape[0], model._vq_vae._num_embeddings).to(device)\n",
        "                encodings.scatter_(1, sample_max_onehot, 1)\n",
        "\n",
        "                # Quantize and unflatten\n",
        "                quantized = torch.matmul(encodings, model._vq_vae._embedding.weight).view(-1, h, w, embedding_dim)\n",
        "\n",
        "                # 生成\n",
        "                quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
        "                generate_max = model._decoder(quantized)\n",
        "\n",
        "                eval_reconstructions = model._decoder(eval_quantize)\n",
        "                \n",
        "                # MSE\n",
        "                eval_img_max_loss_recon.append(F.mse_loss(generate_max.data, eval_reconstructions).item()) # 画像のMSE\n",
        "                eval_img_max_loss.append(F.mse_loss(generate_max.data, eval_originals).item()) # 画像のMSE\n",
        "                # インデックスの一致度\n",
        "                index_match_degree.append((torch.mean((sample_max==sample).float())).item())\n",
        "\n",
        "        print(f'match:{np.mean(index_match_degree)*100}%')\n",
        "        print(f'mse_original:{np.mean(eval_img_max_loss)}')\n",
        "        print(f'mse_reconstructions:{np.mean(eval_img_max_loss_recon)}')\n",
        "\n",
        "\n",
        "df = pd.DataFrame(train_losses, columns=['loss'])\n",
        "df.to_csv('{}/loss.csv'.format(folder))\n",
        "# df = pd.DataFrame(test_losses, columns=['epoch', 'test_error'])\n",
        "# df.to_csv(f'{folder}/test_error.csv')\n",
        "\n",
        "f = plt.figure() #  (default: [6.4, 4.8]) Width, height in inches.\n",
        "ax = f.add_subplot(1,1,1) # 1行1列の1つめ\n",
        "ax.plot(train_losses, marker=\"o\")\n",
        "ax.set_facecolor(\"white\")\n",
        "# ax.set_yscale('')\n",
        "ax.grid()\n",
        "ax.set_title('PixelCNN loss')\n",
        "ax.set_xlabel('epoch')\n",
        "ax.set_ylim(0, train_losses[1]*1.25)\n",
        "plt.savefig(f'{folder}/loss.png', transparent=False)\n",
        "\n",
        "# f = plt.figure() #  (default: [6.4, 4.8]) Width, height in inches.\n",
        "# ax = f.add_subplot(1,1,1) # 1行2列の1つめ\n",
        "# ax.plot(np.array(test_losses)[:,0], np.array(test_losses)[:,1], marker=\"o\")\n",
        "# # ax.set_yscale('')\n",
        "# ax.set_title('PixelCNN testloss')\n",
        "# ax.set_xlabel('epoch')\n",
        "# plt.savefig(f'{folder}/testloss.png')\n",
        "\n",
        "sample = torch.Tensor(img_num, 1, data_recon.shape[2], data_recon.shape[3]).to(device) # 生成用\n",
        "sample.fill_(0)\n",
        "\n",
        "for i in range(data_recon.shape[2]):\n",
        "    for j in range(data_recon.shape[3]):\n",
        "        out = pixel_cnn(sample).to(device)\n",
        "        probs = F.softmax(out[:, :, i, j], dim=1)\n",
        "        sample[:, :, i, j] = torch.multinomial(probs, 1).float()\n",
        "\n",
        "sample = sample.long()\n",
        "sample = sample.view(-1).unsqueeze(1)\n",
        "\n",
        "\n",
        "encodings = torch.zeros(sample.shape[0], model._vq_vae._num_embeddings).to(device)\n",
        "encodings.scatter_(1, sample, 1)\n",
        "\n",
        "# Quantize and unflatten\n",
        "quantized = torch.matmul(encodings, model._vq_vae._embedding.weight).view(-1, data_recon.shape[2], data_recon.shape[3], 64)\n",
        "\n",
        "# 生成\n",
        "quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
        "generate = model._decoder(quantized)\n",
        "\n",
        "save_image(generate.data, f\"{folder}/k_{num_embeddings}.png\", nrow=5, padding=0,  normalize=True)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "vq-vae.ipynb のコピー",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('test')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "71de5d439efc69c58b86ea65c5e8d20e89004ad94ea73b48b827b80c56ff4491"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
